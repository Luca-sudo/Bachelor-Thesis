\chapter{Preliminaries} \label{chap:preliminaries}
\begin{itemize}
	\item Amortized analysis potential method
	\item Type Theory basics
	   \begin{itemize}
		\item Judgements 
		\item Context
		\item Universe
		\item Inference rules?
	   \end{itemize}
   \item Linear type systems: what it is and why we want it for this thesis.
   \item Functional Programming constraints (For those that only ever programmed imperatively)
   \item Linear programming, and how it relates to the thesis
   \item 
\end{itemize}

\section{Amortized Analysis}

Analysis of algorithms centers around two characteristics: correctness and cost. Especially the analysis of costs is a vast field with various approaches including, but not limited to, worst-case analysis, average-case analysis and amortized analysis - with each serving a nuanced purpose. Worst-case analysis is familiar to most computer science students, as it is subject of courses taught throughout their bachelor. While it is useful to reason about the worst-case performance of algorithms, it is potentially to pessimistic when compared against concrete applications. 
\todo{Add reference to example}

Average-case analysis provides a paradigm shift. Instead of examining \emph{only} the worst-case, the algorithm is analyzed with respect to the probability distribution of the respective inputs. It thus represents a more nuanced view of the problem domain, taking into account the varying performance and the probability for the input. 

Amortized analysis approaches the question of cost differently. Instead of evaluating the cost ofa single operation, it considers the cost of a \emph{sequence} of operations. This may provide an analysis that is more akin to the concrete usage of algorithms - rarely is an algorithm executed once.

To illustrate the profound difference, consider a list data structure that is implemented with an underlying array. Whenever an insert operation fills the array, it doubles in size. Worst-case analysis would ascribe linear cost (\(\mathcal{O}(n)\), where \(n\) is the length of the list) to an insert operation, because the worst-case entails resizing the array. Amortized analysis on the other hand would analyze a sequence of \(n\) insert operations.

We assume that inserting into an array takes constant time, if the array has free space. Then, inserting \(n + 1\) elements into an array of size \(n\) requires \(n\) constant time inserts, as well as one resizing of the array, which requires \(\mathcal{O}(n)\) time. Averaged out over \(n + 1\) operations, the insert operation requires only constant time. 

While there are various approaches to amortized analysis, we use the potential method \todo{reference} throughout this thesis. In the above example, we allow the list to store a potential, depending on it's size, which can be used to amortize more expensive operations. For this, we define a potential function \(\Phi(l) = 2\cdot |l| - N\), where \(|l|\) refers to the number of elements stored in the list, and \(N\) refers to the maximum capacity of \(l\). The amortized cost of any operation is composed of the actual cost of the operation and the change in potential.

Assuming \(|l| < N\), inserting an element in the list has an actual constant cost, and the potential of the list is incremented by \(2\). As a result, the amortized cost, which is the sum of actual cost and change in potential, is also constant. 

Assuming \(|l| = N\), the array has to double in size. The resulting potential after insertion is thus \(2\), as \(|l| = N + 1\). Prior to insertion, the potential was \(|l|\), as \(|l| = N \implies 2 \cdot |l| - N = |l|\). Thus, a potential linear in the size of the list is freed, which is used to amortize the actual cost of insertion - yielding an amortized constant cost.


\section{Purely Functional Programming}

In this thesis, we constrain ourselves to introducing purely functional programming languages, for a couple of reasons. Reasoning about such programs avoids obstacles common in imperative programming languages, such as mutation. Furthermore, we can write concise code, which allows us to focus on introducing and motivating concepts more thoroughly. For the reader who has only a vague idea of what constitutes purely functional programming, we introduce the key characteristics, at times comparing them to imperative programming for illustration.

\subsection{Immutability}

At its core, functional programming centers around the concept of treating computation as the evaluation of mathematical functions. Those functions are commonly referred to as "black boxes", which given some input produce some output. Compared to imperative programming, where computation comprises sequential manipulation of state, functional programming regards data as immutable. This provides advantages, especially for parallel and concurrent computations, as race conditions and deadlocks are eliminated. This is achieved at the expense of memory and performance, as partially mutating large data structures requires less resources and computation than reconstructing the entire data structure.

\subsection{Type Signatures}

There are two parts to defining a function in a mathematical setting. First, we need to define how individual values are mapped. Secondly, we need to define the domain and co-domain of the function - these are the sets of values the function allows as input or output, respectively. In functional programming, functions also have an associated domain and co-domain. Assuming that the type of integers is denoted \(\typeint\), a function that given an integer increments and returns it, has a \emph{type signature} \(\typeint \to \typeint\).  

Functions can take multiple arguments - of possibly distinct types. To model type signatures with multiple arguments we use \emph{currying}. Consider a function add that returns the sum of two integers. One possible way of defining the domain is using the cartesian product. Thus, the domain could be \(\typeint \times \typeint\). \emph{Currying} the function then means converting a \(k\)-ary function into a sequence of \emph{unary} functions. In the above example, this results in the following type signature: \(\typeint \to \typeint \to \typeint\). Thus, the function first takes an integer, followed by the second integer. 

\todo{Illustration for currying that example}

\subsection{Higher-Order Functions}

One of the hallmarks of functional programming is its compositionality and high level of abstraction. Treating computation as functions facilitates compositionality, as functions can - under certain conditions - be composed easily. Thus, one can construct complex computation from a set of primitive functions, by composing them in intricate ways. Because functions are integral to functional programming, they are treated equally to any other data; Functions can, therefore, be passed to, and returned from, functions. Functions that take or return other functions are called \emph{higher-order} functions. As a canonical example, we illustrate the higher-order function map.

Map is a function that operates on lists, taking a function \(f\) and a list \(l\) as its input. The function \(f\) is then applied to every element in the list. 

\todo{Illustration of map}







