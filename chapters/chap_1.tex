\chapter{Preliminaries} \label{chap:preliminaries}

There are various concepts underpinning AARA, which we introduce briefly in this section. Amortized analysis (\cref{sec:amortized-analysis}) provides a foundational notion of cost. Type theoretic notions (\cref{sec:type-theory}) allow reasoning about resource demands of programs without executing them. Furthermore, we need to solve a system of linear constraints, to retrieve appropriate resource bounds for programs. This requires linear programming, which we introduce in \cref{sec:linear-programming}. 

\section{Amortized Analysis}\label{sec:amortized-analysis}

Analysis of algorithms centers around two characteristics: correctness and cost. Especially the analysis of costs is a vast field with various approaches including, but not limited to, worst-case analysis, average-case analysis and amortized analysis - with each serving a nuanced purpose. Worst-case analysis is familiar to most computer science students, as it is subject of courses taught throughout their bachelor. While it is useful to reason about the worst-case performance of algorithms, it is potentially too pessimistic when compared against concrete applications. 
\todo{Add reference to example}

Average-case analysis provides a paradigm shift. Instead of examining \emph{only} the worst-case, the algorithm is analyzed with respect to the probability distribution of the respective inputs. It thus represents a more nuanced view of the problem domain, taking into account the varying performance and the probability for the input. 

Amortized analysis approaches the question of cost differently. Instead of evaluating the cost of a single operation, it considers the cost of a \emph{sequence} of operations. This may provide an analysis that is more akin to the concrete usage of algorithms - rarely is an algorithm executed once.

To illustrate the profound difference, consider a list data structure that is implemented with an underlying array. Whenever an insert operation fills the array, it doubles in size. Worst-case analysis would ascribe linear cost (\(\mathcal{O}(n)\), where \(n\) is the length of the list) to an insert operation, because the worst-case entails resizing the array. Amortized analysis on the other hand would analyze a sequence of \(n\) insert operations.

We assume that inserting into an array takes constant time, if the array has free space. Then, inserting \(n + 1\) elements into an array of size \(n\) requires \(n\) constant time inserts, as well as one resizing of the array, which requires \(\mathcal{O}(n)\) time. Averaged out over \(n + 1\) operations, the insert operation requires only constant time. 

While there are various approaches to amortized analysis, we use the potential method, which is introduced in \cref{sec:resource-aware-lists}, to allow elucidation of resource bounds respecting variable list sizes. In the above example, we allow the list to store a potential, depending on it's size, which can be used to amortize more expensive operations. For this, we define a potential function \(\Phi(l) = 2\cdot |l| - N\), where \(|l|\) refers to the number of elements stored in the list, and \(N\) refers to the maximum capacity of \(l\). The amortized cost of any operation is composed of the actual cost of the operation and the change in potential.

Assuming \(|l| < N\), inserting an element in the list has an actual constant cost, and the potential of the list is incremented by \(2\). As a result, the amortized cost, which is the sum of actual cost and change in potential, is also constant. 

Assuming \(|l| = N\), the array has to double in size. The resulting potential after insertion is thus \(2\), as \(|l| = N + 1\). Prior to insertion, the potential was \(|l|\), as \(|l| = N \implies 2 \cdot |l| - N = |l|\). Thus, a potential linear in the size of the list is freed, which is used to amortize the actual cost of insertion - yielding an amortized constant cost.

\section{Type Theory}\label{sec:type-theory}

Most computer scientists are exposed to Zermelo-Fraenkel set theory (ZFC) throughout mathematics courses during their studies. Type theory, a foundational concept in computer science and formal logic, offers a paradigm shift, compared to ZFC. Differing from ZFC in several fundamental ways, type theory provides an alternative framework that focuses on the classification and interaction of values based on their \emph{types}.

While ZFC revolves around sets as its central objects, type theory revolves around the notion of types, which categorize values into different groups. Most readers are already familiar with types, as they are ubiquitous in programming. In C for example, the user has to define the type of a variable upon initialization. It is not possible to assign a string of characters to a variable of type \typeint.

\subsection{Inference Rules}

Type judgements are a fundamental concept in type theory, indicating that an expression has a certain type. To state that the variable \(x\) has type \(\typeint\), we write \(x : \typeint\). One may think of a typing judgement as an analogue to the \(\in\) operator in ZFC. Fundamentally, \emph{type systems} provide rules to reason about types of complex expressions.

These rules are called \emph{inference rules} and allow us to assign types to expressions, given the required \emph{premises} hold. Premises can be, among many, constraints and typing judgements themselves. The most common notation for inference rules looks like this: 

\[
   \inference[(Name)]
   {Premise_1 \quad Premise_2}
   {Conclusion}
\]

Expressions are typed by sequential application of defined inference rules. The visual representation generated by such sequential application is called a \emph{type derivation}. In \cref{sec:example-type-derivations} we provide type derivations for select examples.

\subsection{Linear Type Systems}

Substructural type systems impose restrictions on the usage of variables and resources. As such, \emph{linear type systems} are a special kind of substructural type system - variables can be used \emph{exactly} once. This abolishes certain classes of errors, such as dangling pointers. For AARA, it streamlines certain correctness proofs and simplifies assignation of potentials to list, as lists cannot be used multiple times.

\subsection{Further Reading}

For readers interested in type theory and programming language theory, we recommend \cite{pierceTypesProgrammingLanguages2002}.

For readers interested in type theory with a focus on formal proofs, we recommend \cite{nederpeltTypeTheoryFormal2014}.

\section{Linear Programming}\label{sec:linear-programming}

Linear programming \cite{LinearProgramming2023} is a special kind of optimization problem \cite{OptimizationProblem2022}. Given a set of \emph{linear} inequalities, we need to find the best solution. Linear programming is a difficult problem - it is in \textbf{NP} \cite[page~56]{aroraComputationalComplexityModern2009}. 

While performing AARA, some type rules introduce numeric constraints. Those are emitted, during type derivation, by specific inference rules. Upon completion of type derivation, all constraints are collected and solved using linear programming. The illustrative examples we choose throughout the thesis emit only a small amount of constraints, which permits solving by hand. When automatizing this procedure, applications utilize an \emph{LP-solver} backend.

\section{Purely Functional Programming}\label{sec:functional-programming}

In this thesis, we constrain ourselves to introducing purely functional programming languages, for a couple of reasons. Reasoning about such programs avoids obstacles common in imperative programming languages, such as mutation. Furthermore, we can write concise code, which allows us to focus on introducing and motivating concepts more thoroughly. For the reader who has only a vague idea of what constitutes purely functional programming, we introduce the key characteristics, at times comparing them to imperative programming for illustration.

\subsection{Immutability}

At its core, functional programming centers around the concept of treating computation as the evaluation of mathematical functions. Those functions are commonly referred to as "black boxes", which given some input produce some output. Compared to imperative programming, where computation comprises sequential manipulation of state, functional programming regards data as immutable. This provides advantages, especially for parallel and concurrent computations, as race conditions and deadlocks are eliminated. This is achieved at the expense of memory and performance, as partially mutating large data structures requires less resources and computation than reconstructing the entire data structure.

\subsection{Type Signatures}

There are two parts to defining a function in a mathematical setting. First, we need to define how individual values are mapped. Secondly, we need to define the domain and co-domain of the function - these are the sets of values the function allows as input or output, respectively. In functional programming, functions also have an associated domain and co-domain. Assuming that the type of integers is denoted \(\typeint\), a function that given an integer increments and returns it, has a \emph{type signature} \(\typeint \to \typeint\).  

Functions can take multiple arguments - of possibly distinct types. To model type signatures with multiple arguments we use \emph{currying}. Consider a function add that returns the sum of two integers. One possible way of defining the domain is using the cartesian product. Thus, the domain could be \(\typeint \times \typeint\). \emph{Currying} the function then means converting a \(k\)-ary function into a sequence of \emph{unary} functions. In the above example, this results in the following type signature: \(\typeint \to \typeint \to \typeint\). Thus, the function first takes an integer, followed by the second integer. 

\todo{Illustration for currying that example}

In \cref{sec:code-examples}, we provide example functions used throughout the thesis, along with their type signatures.

\subsection{Higher-Order Functions}

One of the hallmarks of functional programming is its compositionality and high level of abstraction. Treating computation as functions facilitates compositionality, as functions can - under certain conditions - be composed easily. Thus, one can construct complex computation from a set of primitive functions, by composing them in intricate ways. Because functions are integral to functional programming, they are treated equally to any other data; Functions can, therefore, be passed to, and returned from, functions. Functions that take or return other functions are called \emph{higher-order} functions. For a canonical example, consider the function \(map\), found in \cref{fig:code-map}.

\todo{Illustration of map}

\subsection{Purity}

Writing imperative code, it is common for function calls to modify some global state. Whether incrementing a counter, writing to a log, etc. - those are \emph{side effects}. A \emph{purely} functional programming language prohibits side effects, which has advantages and disadvantages. As side effects are common sources of bugs, eliminating them potentially makes code less error-prone. Conversely, this requires the introduction of new tools to handle side effects. However, of paramount significance to this thesis is that the absence of side effects and global state simplifies the elucidation of resource requirements. 

\subsection{Further Reading}

For a thorough introduction into functional programming, we recommend \cite{allenHaskellProgrammingFirst2016}. It uses the Haskell programming language \cite{HaskellLanguage}.
